"""
Vision Transformer based regressor with reference-guided architecture.
æ ¸å¿ƒæ€æƒ³ï¼šç›´æ¥å»ºæ¨¡ç”Ÿæˆå›¾åƒç›¸å¯¹äºåŸå§‹å›¾åƒçš„å·®å¼‚
"""
import torch
import torch.nn as nn
import timm


class ReferenceGuidedViT(nn.Module):
    """
    å‚è€ƒå¼•å¯¼çš„ViTæ¶æ„
    
    æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š
    1. å…±äº«backboneæå–ç‰¹å¾ï¼ˆé¿å…å‚æ•°å†—ä½™ï¼‰
    2. ç›´æ¥å»ºæ¨¡ç‰¹å¾å·®å¼‚ï¼ˆgen - rawï¼‰
    3. ç»“åˆç»å¯¹ç‰¹å¾å’Œç›¸å¯¹å·®å¼‚è¿›è¡Œé¢„æµ‹
    
    ä¼˜åŠ¿ï¼š
    - å‚æ•°é«˜æ•ˆï¼šåªæœ‰ä¸€ä¸ªViT backbone
    - è®¡ç®—é«˜æ•ˆï¼šå¯ä»¥batchå¤„ç†ä¸¤ç±»å›¾åƒ
    - è¯­ä¹‰æ¸…æ™°ï¼šæ˜¾å¼å»ºæ¨¡"ç›¸å¯¹äºåŸå§‹å›¾åƒçš„è´¨é‡å·®å¼‚"
    """
    
    def __init__(
        self,
        model_name='vit_base_patch16_224',
        pretrained=False,
        embedding_dim=256,
        face_pretrained_path=None,
        freeze_backbone=False,
    ):
        super().__init__()
        
        # å…±äº«backbone
        # å¤„ç† hf-hub: å‰ç¼€
        if model_name.startswith('hf-hub:'):
            actual_model_name = model_name.replace('hf-hub:', '')
        else:
            actual_model_name = model_name
            
        self.backbone = timm.create_model(
            actual_model_name,
            pretrained=pretrained,
            num_classes=0,
        )
        
        # å†»ç»“backboneï¼ˆå°æ•°æ®é›†å…³é”®ç­–ç•¥ï¼‰
        if freeze_backbone:
            print("ğŸ”’ å†»ç»“backboneå‚æ•°ï¼Œåªè®­ç»ƒä»»åŠ¡å¤´")
            for param in self.backbone.parameters():
                param.requires_grad = False
        
        # åŠ è½½äººè„¸é¢„è®­ç»ƒæƒé‡
        if face_pretrained_path and face_pretrained_path.lower() != 'none':
            print(f"åŠ è½½äººè„¸é¢„è®­ç»ƒæƒé‡: {face_pretrained_path}")
            try:
                state_dict = torch.load(face_pretrained_path, map_location='cpu')
                if 'model' in state_dict:
                    state_dict = state_dict['model']
                elif 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                
                self.backbone.load_state_dict(state_dict, strict=False)
                print("âœ“ æƒé‡åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš  è­¦å‘Š: æƒé‡åŠ è½½å¤±è´¥: {e}")
        
        self.feature_dim = self.backbone.num_features
        
        # å·®å¼‚å»ºæ¨¡å±‚ï¼šå­¦ä¹ å¦‚ä½•åˆ©ç”¨ç‰¹å¾å·®å¼‚
        self.diff_encoder = nn.Sequential(
            nn.Linear(self.feature_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.1),
        )
        
        # ç‰¹å¾èåˆï¼šç»“åˆç»å¯¹ç‰¹å¾å’Œç›¸å¯¹å·®å¼‚
        # è¾“å…¥ï¼š[gen_feat, raw_feat, diff_feat] -> 3 * feature_dim
        self.fusion = nn.Sequential(
            nn.Linear(self.feature_dim + 512, embedding_dim),
            nn.LayerNorm(embedding_dim),
            nn.GELU(),
            nn.Dropout(0.2),
        )
        
        # ä»»åŠ¡å¤´
        self.quality_head = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1),
        )
        
        self.identity_head = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1),
        )
    
    def forward(self, gen_images, raw_images, return_embedding=False):
        """
        Args:
            gen_images: Generated images (batch, 3, H, W)
            raw_images: Raw reference images (batch, 3, H, W)
            return_embedding: Whether to return embedding
        
        Returns:
            quality_pred, identity_pred, [embeddings]
        """
        batch_size = gen_images.size(0)
        
        # é«˜æ•ˆbatchå¤„ç†ï¼šå°†ç”Ÿæˆå›¾åƒå’ŒåŸå§‹å›¾åƒæ‹¼æ¥åœ¨ä¸€èµ·
        # [gen_1, gen_2, ..., gen_n, raw_1, raw_2, ..., raw_n]
        combined_images = torch.cat([gen_images, raw_images], dim=0)  # (2*batch, 3, H, W)
        
        # ä¸€æ¬¡forwardæå–æ‰€æœ‰ç‰¹å¾
        combined_features = self.backbone(combined_images)  # (2*batch, feature_dim)
        
        # åˆ†ç¦»ç”Ÿæˆå›¾åƒå’ŒåŸå§‹å›¾åƒçš„ç‰¹å¾
        gen_feat = combined_features[:batch_size]  # (batch, feature_dim)
        raw_feat = combined_features[batch_size:]  # (batch, feature_dim)
        
        # è®¡ç®—ç‰¹å¾å·®å¼‚ï¼ˆæ ¸å¿ƒï¼šæ˜¾å¼å»ºæ¨¡ç›¸å¯¹å·®å¼‚ï¼‰
        diff_feat = gen_feat - raw_feat  # (batch, feature_dim)
        
        # ç¼–ç å·®å¼‚ç‰¹å¾
        diff_encoded = self.diff_encoder(diff_feat)  # (batch, 512)
        
        # èåˆï¼šç”Ÿæˆå›¾åƒç‰¹å¾ + å·®å¼‚ç‰¹å¾
        # åŸå§‹å›¾åƒç‰¹å¾ä½œä¸º"é”šç‚¹"å·²ç»éšå«åœ¨å·®å¼‚ä¸­ï¼Œä¸éœ€è¦æ˜¾å¼ä½¿ç”¨
        fused_feat = torch.cat([gen_feat, diff_encoded], dim=1)  # (batch, feature_dim + 512)
        
        # è·å–æœ€ç»ˆembedding
        embedding = self.fusion(fused_feat)  # (batch, embedding_dim)
        embedding_norm = nn.functional.normalize(embedding, p=2, dim=1)
        
        # é¢„æµ‹åˆ†æ•°
        quality_pred = self.quality_head(embedding).squeeze(-1)
        identity_pred = self.identity_head(embedding).squeeze(-1)
        
        if return_embedding:
            return quality_pred, identity_pred, embedding_norm
        else:
            return quality_pred, identity_pred


class SingleBranchViT(nn.Module):
    """Single-branch ViT (baseline, only uses generated images)."""
    
    def __init__(
        self,
        model_name='vit_base_patch16_224',
        pretrained=False,
        embedding_dim=256,
        face_pretrained_path=None,
    ):
        super().__init__()
        
        # å¤„ç† hf-hub: å‰ç¼€
        if model_name.startswith('hf-hub:'):
            actual_model_name = model_name.replace('hf-hub:', '')
        else:
            actual_model_name = model_name
            
        self.backbone = timm.create_model(
            actual_model_name,
            pretrained=pretrained,
            num_classes=0,
        )
        
        if face_pretrained_path and face_pretrained_path.lower() != 'none':
            print(f"åŠ è½½äººè„¸é¢„è®­ç»ƒæƒé‡: {face_pretrained_path}")
            try:
                state_dict = torch.load(face_pretrained_path, map_location='cpu')
                if 'model' in state_dict:
                    state_dict = state_dict['model']
                elif 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                
                self.backbone.load_state_dict(state_dict, strict=False)
                print("âœ“ æƒé‡åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš  è­¦å‘Š: æƒé‡åŠ è½½å¤±è´¥: {e}")
        
        self.feature_dim = self.backbone.num_features
        
        self.embedding_layer = nn.Sequential(
            nn.Linear(self.feature_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(512, embedding_dim),
            nn.LayerNorm(embedding_dim),
        )
        
        self.quality_head = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1),
        )
        
        self.identity_head = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1),
        )
    
    def forward(self, gen_images, return_embedding=False):
        feat = self.backbone(gen_images)
        embedding = self.embedding_layer(feat)
        embedding_norm = nn.functional.normalize(embedding, p=2, dim=1)
        
        quality_pred = self.quality_head(embedding).squeeze(-1)
        identity_pred = self.identity_head(embedding).squeeze(-1)
        
        if return_embedding:
            return quality_pred, identity_pred, embedding_norm
        else:
            return quality_pred, identity_pred
